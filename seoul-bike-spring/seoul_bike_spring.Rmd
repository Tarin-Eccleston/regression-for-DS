## STATS 762: Assignment 3
### Author: Tarin Eccleston

NOTES
- Low mse, however high p-value could be due to multicol, or high variances compared to mean.
- Need to standardardise the features...

Rental bikes are introduced in many urban cities for the enhancement of mobility comfort. It is important to make the rental bike available and accessible to the public at the right time. In order to provide the city with a stable supply
of rental bikes, a reliable model for rented bike counts at each hour and weather is required.

The spreadsheet SeoulBikeSpring.csv contains rented bike counts at each hour and weather information in Seoul, Korea.

```{r}
library(tidyverse)
library(ggplot2)
library(glmnet)
library(pscl)
library(quantreg)

setwd("/Users/tarineccleston/Documents/Masters/STATS 762/regression-for-DS/seoul-bike-spring")

seoul_bike_df = as.data.frame(read.csv("data/SeoulBikeSpring.csv"))
next_day_df = as.data.frame(read.csv("data/nextday.csv"))

set.seed(991)
```


#### 1a) Why snowfall is not useful in modelling rented bike counts and there is no need to include snowfall in a model?

```{r}
ggplot(seoul_bike_df, aes(x = Snowfall)) +
  geom_histogram() +
  labs(x = "Snowfall (cm)", y = "Frequency") +
  xlim(-1, 10)
```

ALL 2136 observations in Spring record 0 cm of snowfall in that particular hour. In regression we are interested in changes of an explanatory variable and it's effect on the response variable. If the values of the snowfall have no variation for all observations, then it is recommend to remove snowfall as it doesn't provide any information and may negatively impact the model's performance.

#### 1b) it an appropriate regression model using all covariates except snow-fall and, find the mean squared error (MSE).

```{r}
# remove snowfall
seoul_bike_df = subset(seoul_bike_df, select = -c(Snowfall))

'''
# train / test split
sample_index = sample(nrow(seoul_bike_df), as.numeric(nrow(seoul_bike_df) * 0.8), replace = FALSE)
seoul_bike_train_df = seoul_bike_df[sample_index,]
seoul_bike_test_df = seoul_bike_df[-sample_index,]
'''

# fit poisson model as we are trying to count the number of rented bikes
# include polynomial terms
seoul_bike_fit = glm(Rented.Bike.Count ~ ., family = "poisson", data = seoul_bike_df)
summary(seoul_bike_fit)
```
```{r}
# Use the fitted model to predict the response variable for the testing data
y_hat = predict(seoul_bike_fit, newdata = seoul_bike_df, type = "response")

# Calculate the MSE for the predicted values
mse = mean((seoul_bike_df$Rented.Bike.Count - y_hat)^2)
mse
```

#### 1c) Fit a parsimonious regression with l1 penalty. List covariates included in the parsimonious model and find the MSE.

```{r}
# use CV on a range of lambda values for lasso regression
cv_lasso = cv.glmnet(as.matrix(seoul_bike_df[,-1]), as.matrix(seoul_bike_df[,1]), alpha=1, standardize=TRUE)
plot(cv_lasso)
```

```{r}
lamda_par = cv_lasso$lambda.1se
lamda_par
```

```{r}
coef(cv_lasso, s= cv_lasso$lambda.1se)
```
Since we use lasso regression, we get a mixture of shrinkage and sparsity of beta coefficients. Therefore we perform variable selection. The included covariates are: Hour, Temperature, Humidity, Visibility and Rainfall.

```{r}
y_hat_par = predict(cv_lasso, newx = as.matrix(seoul_bike_df[,-1]), s = lamda_par, type = "response")

# Calculate the MSE for the predicted values
mse_par <- mean((seoul_bike_df$Rented.Bike.Count - y_hat_par)^2)
mse_par
```

#### 1d) Using your parsimonious model in (c), predict rented bike counts
on the next day (nextday.csv). List your prediction against actual
counts on the next day and find at which hour, your prediction is the
best.

```{r}
next_day_df = subset(next_day_df, select = -c(Snowfall))
  
# use parsimonious model with l1 regularization
next_day_y_hat = predict(cv_lasso, newx = as.matrix(next_day_df[,-1]), s = lamda_par, type = "response")
colnames(next_day_y_hat) = "next_day_y_hat"

mse =  mean((next_day_df$Rented.Bike.Count - next_day_y_hat)^2)
mse
```

```{r}
# combine dataframes for plotting
next_day_y = as.data.frame(next_day_df$Rented.Bike.Count)
colnames(next_day_y) = "next_day_y"
next_day_y_hat = as.data.frame(next_day_y_hat)

next_day_comparisons = next_day_y %>%
  mutate(next_day_y_hat) %>%
  mutate(Hour = next_day_df$Hour) %>%
  relocate(Hour, .before = next_day_y)

ggplot(next_day_comparisons, aes(x = Hour)) +
  geom_area(aes(y = next_day_y, fill = "Actual Bike Rent Count"), alpha = 0.5) +
  geom_area(aes(y = next_day_y_hat, fill = "Predicted Bike Rent Count"), alpha = 0.5) +
  labs(x = "Hour", y = "Bike Rent Count", fill = "", color = "") +
  ggtitle("Comparison between Actual and Predicted Bike Rent Counts") +
  scale_fill_manual(name = "", values = c("Actual Bike Rent Count" = "grey", "Predicted Bike Rent Count" = "red")) +
  scale_color_manual(name = "", values = c("Actual Bike Rent Count" = "black", "Predicted Bike Rent Count" = "black")) +
  theme(legend.position = "bottom")
```
##### Normalize accuracy to get a percentage error

```{r}
next_day_comparisons = next_day_comparisons %>%
  mutate(error_percent = abs(next_day_y - next_day_y_hat)/next_day_y)
```

##### Plot Error Percentage

```{r}
ggplot(next_day_comparisons, aes(x = Hour)) +
  geom_area(aes(y = error_percent), alpha = 0.5) +
  labs(x = "Hour", y = "Prediction Error Percentage (%)", fill = "", color = "") +
  ggtitle("Comparison between Hour and Prediction Error Percentage")
```


```{r}
most_accurate_hour = next_day_comparisons$Hour[which.min(next_day_comparisons$error_percent)]
most_accurate_hour
```

The predicted bike rent count appears to be most accurate at 4 pm.

#### 1e) Your friend claimed that there might be excess zero counts in particular hours or weather conditions and, it is worth to consider a more robust model. Using the same selection of covariates as in your parsimonious model in (c), validate your friendâ€™s claim and describe how covariates affect rented bike counts. When you fit a more robust model for excess zeros, you will need to standardized covariates.

```{r}
# use variables from the parsimonious model
seoul_bike_df = subset(seoul_bike_df, select = -c(Wind.speed, Dew.point.temperature, Solar.Radiation))

# plot counts
hist(seoul_bike_df$Rented.Bike.Count, breaks = seq(min(seoul_bike_df$Rented.Bike.Count), max(seoul_bike_df$Rented.Bike.Count) + 10, by = 10),
     xlab = "Number of Rented Bikes", main = "Histogram of Rented Bike Count in Seoul")
```
There appears to be a clear inflation of zeros. I used the hurdle model to account for zero-inflated data as we are only getting the inflated zeros from one source (covariates)

##### Hurdle

```{r}
# standardize covariates
seoul_bike_df[,-1] = scale(seoul_bike_df[,-1])

seoul_bike_hurdle = hurdle(Rented.Bike.Count ~ ., data = seoul_bike_df, dist = 'poisson', link = 'logit')
summary(seoul_bike_hurdle)
```

All variables appears significant in the truncated poisson model with count > 0. The p-values are all significantly low, at <2e-16. For the the zero-hurdle model (binomial) all covariates have large p-values of over 0.9, apart from temperature and humidity with values of 0.154 and 0.214 respectively. Although the p-value for temperature is relatively larger than 0.05, we can still reject the null hypothesis and say that temperature has an effect on the rented bike counts. Low and high temperatures typically indicate unfavorable conditions for cycling therefore zero-inflation would be expected for rented bike counts.

________________________________________________________________________________________________________

The spreadsheet SeoulBikeSpring.csv excluding bike counts is the hourly
weather record in Seoul. We now wish to model the temperature with
other weather information and hour. For Question 2, the rented bike
counts is excluded.

#### 2a) Is snowfall still not useful in modelling temperature?

```{r}
seoul_spring_weather_df = as.data.frame(read.csv("data/SeoulBikeSpring.csv"))

# reorganise weather dataframe
seoul_spring_weather_df = seoul_spring_weather_df[,-1]
seoul_spring_weather_df = seoul_spring_weather_df %>%
  relocate(Temperature, .before = "Hour")
```

We are now modelling temperature. Even though temperature is much more related to snowfall due to physical relationships between them, it is still recommended to remove snowfall as it has no variation, i.e., all the values for that variable are the same, as they do not provide any information and may negatively impact the model's performance.

#### 2b) Fit a parsimonious regression with l1 penalty. List covariates included in the parsimonious model and find the MSE. 

```{r}
# use CV on a range of lambda values for lasso regression
cv_lasso = cv.glmnet(as.matrix(seoul_spring_weather_df[,-1]), as.matrix(seoul_spring_weather_df[,1]), alpha=1, standardize=TRUE)
plot(cv_lasso)
```

```{r}
lamda_par = cv_lasso$lambda.1se
lamda_par
```

```{r}
coef(cv_lasso, s= cv_lasso$lambda.1se)
```
Since we use lasso regression, we get a mixture of shrinkage and sparsity of beta coefficients. Therefore we perform variable selection. The included covariates are: Hour, Humidity, Dew.point.temperature and Solar.Radiation.

```{r}
y_hat_par = predict(cv_lasso, newx = as.matrix(seoul_spring_weather_df[,-1]), s = lamda_par, type = "response")

# Calculate the MSE for the predicted values
mse_par <- mean((seoul_spring_weather_df$Temperature - y_hat_par)^2)
mse_par
```

#### 2c) Using the same selection of covariates as in your parsimonious model
in (b), fit quantile regressions for median and lower/upper quartile
temperatures. Present boxplot of median and lower/upper quartile
temperatures against hours. Describe the two points in the relation
between covariates and conditional distribution for temperature.

```{r}
ql25 <- rq(Temperature ~ Hour + Humidity + Dew.point.temperature + Solar.Radiation, data = seoul_spring_weather_df, tau = 0.25)
ql50 <- rq(Temperature ~ Hour + Humidity + Dew.point.temperature + Solar.Radiation, data = seoul_spring_weather_df, tau = 0.50)
ql75 <- rq(Temperature ~ Hour + Humidity + Dew.point.temperature + Solar.Radiation, data = seoul_spring_weather_df, tau = 0.75)
```

```{r}
ggplot(seoul_spring_weather_df, aes(x = as.factor(Hour), y = Temperature)) + 
  geom_boxplot() +
  ggtitle("Hour of Day vs Temperature in Seoul Spring") +
  xlab("Hours")
```
- The effect of hour on temperature is nonlinear. The boxplots show that temperature has a U-shaped relationship with Hour, with higher temperatures in the late morning to the late afternoon, and lower temperatures in the early evening to the late morning. *The quantile regression lines confirm this, showing that the effect of hour on temperature is nonlinear for all three quantiles.

- Looking at the median, upper and lower quantiles, temperatures appear normally distributed during cooler times in the day (early evening to the late morning), however temperatures appear to be left-skewed for hotter times of the day (late morning to the late afternoon). 

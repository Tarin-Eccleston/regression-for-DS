## STATS 762: Assignment 3
### Author: Tarin Eccleston

NOTES
- Low mse, however high p-value could be due to multicol, or high variances compared to mean.
- Need to standardardise the features...

Rental bikes are introduced in many urban cities for the enhancement of mobility comfort. It is important to make the rental bike available and accessible to the public at the right time. In order to provide the city with a stable supply
of rental bikes, a reliable model for rented bike counts at each hour and weather is required.

The spreadsheet SeoulBikeSpring.csv contains rented bike counts at each hour and weather information in Seoul, Korea.

```{r}
library(tidyverse)
library(ggplot2)
library(glmnet)
library(pscl)
library(quantreg)

setwd("/Users/tarineccleston/Documents/Masters/STATS 762/regression-for-DS/seoul-bike-spring")

seoul_bike_df = as.data.frame(read.csv("data/SeoulBikeSpring.csv"))
next_day_df = as.data.frame(read.csv("data/nextday.csv"))

set.seed(991)
```


#### 1a) Why snowfall is not useful in modelling rented bike counts and there is no need to include snowfall in a model?

```{r}
ggplot(seoul_bike_df, aes(x = Snowfall)) +
  geom_histogram() +
  labs(x = "Snowfall (cm)", y = "Frequency") +
  xlim(-1, 10)
```

ALL 2136 observations in Spring record 0 cm of snowfall in that particular hour. In regression we are interested in changes of an explanatory variable and it's effect on the response variable. If the values of the snowfall are the same for all observations, then it would not be useful to model rented bike counts.

#### 1b) it an appropriate regression model using all covariates except snow-fall and, find the mean squared error (MSE).

```{r}
# remove snowfall
seoul_bike_df = subset(seoul_bike_df, select = -c(Snowfall))

'''
# train / test split
sample_index = sample(nrow(seoul_bike_df), as.numeric(nrow(seoul_bike_df) * 0.8), replace = FALSE)
seoul_bike_train_df = seoul_bike_df[sample_index,]
seoul_bike_test_df = seoul_bike_df[-sample_index,]
'''

# fit poisson model as we are trying to count the number of rented bikes
# include polynomial terms
seoul_bike_fit = glm(Rented.Bike.Count ~ ., family = "poisson", data = seoul_bike_df)
summary(seoul_bike_fit)
```
```{r}
# Use the fitted model to predict the response variable for the testing data
y_hat = predict(seoul_bike_fit, newdata = seoul_bike_df, type = "response")

# Calculate the MSE for the predicted values
mse = mean((seoul_bike_df$Rented.Bike.Count - y_hat)^2)
mse
```

#### 1c) Fit a parsimonious regression with l1 penalty. List covariates included in the parsimonious model and find the MSE.

```{r}
# use CV on a range of lambda values for lasso regression
cv_lasso = cv.glmnet(as.matrix(seoul_bike_df[,-1]), as.matrix(seoul_bike_df[,1]), alpha=1, standardize=TRUE)
plot(cv_lasso)
```

```{r}
lamda_par = cv_lasso$lambda.1se
lamda_par
```

```{r}
coef(cv_lasso, s= cv_lasso$lambda.1se)
```
Since we use lasso regression, we get a mixture of shrinkage and sparsity of beta coefficients. Therefore we perform variable selection. The included covariates are: Hour, Temperature, Humidity, Visibility and Rainfall.

```{r}
y_hat_par = predict(cv_lasso, newx = as.matrix(seoul_bike_df[,-1]), s = lamda_par, type = "response")

# Calculate the MSE for the predicted values
mse_par <- mean((seoul_bike_df$Rented.Bike.Count - y_hat_par)^2)
mse_par
```

#### 1d) Using your parsimonious model in (c), predict rented bike counts
on the next day (nextday.csv). List your prediction against actual
counts on the next day and find at which hour, your prediction is the
best.

```{r}
next_day_df = subset(next_day_df, select = -c(Snowfall))
  
# use parsimonious model with l1 regularization
next_day_y_hat = predict(cv_lasso, newx = as.matrix(next_day_df[,-1]), s = lamda_par, type = "response")
colnames(next_day_y_hat) = "next_day_y_hat"

mse =  mean((next_day_df$Rented.Bike.Count - next_day_y_hat)^2)
mse
```

```{r}
# combine dataframes for plotting
next_day_y = as.data.frame(next_day_df$Rented.Bike.Count)
colnames(next_day_y) = "next_day_y"
next_day_y_hat = as.data.frame(next_day_y_hat)

next_day_comparisons = next_day_y %>%
  mutate(next_day_y_hat) %>%
  mutate(Hour = next_day_df$Hour) %>%
  relocate(Hour, .before = next_day_y)

ggplot(next_day_comparisons, aes(x = Hour)) +
  geom_area(aes(y = next_day_y, fill = "Actual Bike Rent Count"), alpha = 0.5) +
  geom_area(aes(y = next_day_y_hat, fill = "Predicted Bike Rent Count"), alpha = 0.5) +
  labs(x = "Hour", y = "Bike Rent Count", fill = "", color = "") +
  ggtitle("Comparison between Actual and Predicted Bike Rent Counts") +
  scale_fill_manual(name = "", values = c("Actual Bike Rent Count" = "grey", "Predicted Bike Rent Count" = "red")) +
  scale_color_manual(name = "", values = c("Actual Bike Rent Count" = "black", "Predicted Bike Rent Count" = "black")) +
  theme(legend.position = "bottom")
```
##### Normalize accuracy to get a percentage error

```{r}
next_day_comparisons = next_day_comparisons %>%
  mutate(error_percent = abs(next_day_y - next_day_y_hat)/next_day_y)
```

##### Plot Error Percentage

```{r}
ggplot(next_day_comparisons, aes(x = Hour)) +
  geom_area(aes(y = error_percent), alpha = 0.5) +
  labs(x = "Hour", y = "Prediction Error Percentage (%)", fill = "", color = "") +
  ggtitle("Comparison between hours and prediction error percentage")
```


```{r}
most_accurate_hour = next_day_comparisons$Hour[which.min(next_day_comparisons$error_percent)]
most_accurate_hour
```

The predicted bike rent count appears to be most accurate at 4 pm.

#### 1e) Your friend claimed that there might be excess zero counts in particular hours or weather conditions and, it is worth to consider a more robust model. Using the same selection of covariates as in your parsimonious model in (c), validate your friendâ€™s claim and describe how covariates affect rented bike counts. When you fit a more robust model for excess zeros, you will need to standardized covariates.

```{r}
# use variables from the parsimonious model
seoul_bike_df = subset(seoul_bike_df, select = -c(Wind.speed, Dew.point.temperature, Solar.Radiation))

# plot counts
hist(seoul_bike_df$Rented.Bike.Count, breaks = seq(min(seoul_bike_df$Rented.Bike.Count), max(seoul_bike_df$Rented.Bike.Count) + 10, by = 10))
```
There appears to be a clear inflation of zeros. Try both zero-inflated poisson and hurdle models to account for this.

##### Hurdle

I used the hurdle model to account for zero-inflated data as we are only getting the inflated zeros from one source (covariates)

```{r}
# zero-inflated count on rented bikes
'''
zinf_seoul_bike <- zeroinfl(Rented.Bike.Count ~ Hour + Temperature + Humidity + Visibility, data = seoul_bike_df, dist='poisson')
summary(zinf_seoul_bike)
'''

# standardise covariates
seoul_bike_df[,-1] = scale(seoul_bike_df[,-1])

seoul_bike_hurdle = hurdle(Rented.Bike.Count ~ ., data = seoul_bike_df, dist = 'poisson', link = 'logit')
summary(seoul_bike_hurdle)
```

Temperature appears to have the most affect on zero-inflation? Lower temperatures indicate unfavourable 

```{r}

```



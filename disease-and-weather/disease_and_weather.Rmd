---
output:
  html_document: default
  pdf_document: default
---

## STATS 762: Assignment 4
### Author: Tarin Eccleston

### Appendix
#### Evaluation Function: Computes classification performance estimates avg accuracy and F1-score 

```{r}
measure.classification = function(class.pred, class.obs, conf.mx){
  entropy = 0 
 # for(i in levels(class.pred)){
  #  entropy = entropy-sum(log(prob.class[class.obs==i,colnames(prob.class)==i])) }
  avg.accuracy=0
  for (i in 1:ncol(conf.mx)){
    avg.accuracy = avg.accuracy + (conf.mx[i,i]+sum(conf.mx[-i,-i]))/sum(conf.mx)
  }
  avg.accuracy=avg.accuracy/i
  precision = mean(diag(conf.mx)/rowSums(conf.mx))
  recall = mean(diag(conf.mx)/colSums(conf.mx))
  F1.score = 2*precision*recall/(precision+recall)
  return(data.frame(avg.accuracy=avg.accuracy,F1.score=F1.score))
}
```

Cardiovascular diseases (CVDs) are one of major causes of death glob-
ally. About 17.9 million lives die each year and this is about 31% of all
deaths worldwide. Heart failure is a common event caused by CVDs and
the dataset train.csv contains 10 features that can be used to predict
mortality by heart failure.

```{r}
set.seed(995)
setwd("/Users/tarineccleston/Documents/Masters/STATS 762/regression-for-DS/disease-and-weather/")
library(rpart)
library(rpart.plot)
library(randomForest)

library(tidyverse)
library(gridExtra)

library(MASS) 
library(klaR)
library(e1071)

library(lubridate)

library(splines)
```

1a) Which explanatory variables are nominal? Which explanatory vari-
ables are numeric? Identify them and modify data to read explana-
tory variables correctly. 

```{r}
cvd_df = read_csv("data/train.csv")
glimpse(cvd_df)
```

Nominal: anaemia, diabetes, high_blood_pressure, sex, smoking, death
Numeric: age, creatinine_phosphokinase, ejection_fraction, platelets, serum_sodium

```{r}
# force to factor
cvd_df$death = as.factor(cvd_df$death)
cvd_df$anaemia = as.factor(cvd_df$anaemia)
cvd_df$diabetes = as.factor(cvd_df$diabetes)
cvd_df$high_blood_pressure = as.factor(cvd_df$high_blood_pressure)
cvd_df$sex = as.factor(cvd_df$sex)
cvd_df$smoking = as.factor(cvd_df$smoking)
```

1b) Fit an appropriate regression model using all explanatory variables.
Write the reason for your model choice. 

```{r}
# use 80/20 train/test split for an unbiased estimate
index = sample(nrow(cvd_df), as.integer(nrow(cvd_df)*0.8), replace = FALSE)
cvd_train_df = cvd_df[index, ]
cvd_test_df = cvd_df[-index, ]

# fit model
cvd_log_fit = glm(death ~ ., data = cvd_train_df, family = 'binomial')
```

##### Logistic Regression

```{r}
death_probs = data.frame(probs = predict(cvd_log_fit, cvd_test_df, type="response"))
death_pred_logistic = ifelse(death_probs > 0.5, 1, 0)

conf_mx_logistic = table(death_pred_logistic, cvd_test_df$death)
conf_mx_logistic
```

```{r}
metric_comparison = data.frame()

metrics_logistic = measure.classification(death_pred_logistic, cvd_test_df$death, conf_mx_logistic)
rownames(metrics_logistic) = "Logistic Regression"

metric_comparison = rbind(metric_comparison, metrics_logistic)
```

Fit logistic regression first as this is the simplest classification model and can be used as a benchmark. We are predicting the likelihood of death (0 or 1) given all other explanatory variables. Probabilities above 0.5 result in death, otherwise anything below 0.5 will result in living.

#### Decision Tree

```{r}
cvd_dt_fit <- rpart(death ~ ., data = cvd_train_df, method = "class", cp=0.001)
cvd_dt_fit$cptable
```

Tree No. 4 provides the best accuracy with a dip in xerror. Find the parsimonious model by adding the standard error, resulting in tree no. 3 with 2 splits. 

```{r}
cp.1se= max(cvd_dt_fit$cptable[cvd_dt_fit$cptable[,4]<sum(cvd_dt_fit$cptable[which.min(cvd_dt_fit$cptable[,4]),c(4,5)]),1])
cp.1se
```

```{r}
cvd_dt_prune_fit <-prune(cvd_dt_fit, cp = cp.1se)
cvd_dt_prune_fit
```

```{r}
death_pred_dt_pruned = predict(cvd_dt_prune_fit, cvd_test_df, type="class")

conf_mx_dt_pruned = table(death_pred_dt_pruned, cvd_test_df$death)
conf_mx_dt_pruned
```

```{r}
metrics_dt = measure.classification(death_pred_dt_pruned, cvd_test_df$death, conf_mx_dt_pruned)
rownames(metrics_dt) = "Decision Tree"

metric_comparison = rbind(metric_comparison, metrics_dt)
```

Decision Tree performs better. This could be due to creating decision boundaries to seperate between positive and negative class labels.

#### Random Forest

```{r}
cvd_rf_fit = randomForest(death ~ ., data = cvd_train_df, importance=TRUE)
plot(cvd_rf_fit)

death_rf_pred = predict(cvd_rf_fit, cvd_test_df, type="class")

conf_mx_rf = table(death_rf_pred, cvd_test_df$death)
conf_mx_rf
```

```{r}
metrics_rf = measure.classification(death_rf_pred, cvd_test_df$death, conf_mx_rf)
rownames(metrics_rf) = "Random Forest"

metric_comparison = rbind(metric_comparison, metrics_rf)
```

```{r}
head(metric_comparison)
```

I used average accuracy as it is a good approximation of model performance. I believe the F1 score is a good measure of overall performance of a model as it is the harmonic mean of the precision and recall scores. Precision measures how well the model predicts positive instances correctly whereas the recall measures how well the model captures positive instances from the entire dataset. A high recall indicates a low number of false negatives.

Logistic regression performed okay, with an accuracy of 78%. Decision tree performed the best with an accuracy of 80%. This could be due to decision trees ability to capture non-linear relationships between the input variables and the target variable. If the underlying data has complex interactions and non-linear patterns, decision trees can potentially model them more effectively than logistic regression, which assumes a linear relationship between the predictors and the log-odds of the target variable. 

Consequently random forests performed the poorest with an accuracy of 73% and F1 score of 65%. This could be due to the data having two inherently important variables for predicting death due to CVD - ejection fraction and age (1c). When these variables are excluded from the splits for certain random trees, this could result in trees which are overfitted to the noise in the data rather than the underlying relationship between death and ejection fraction and age.

In conclusion, decision trees perform the best out of the three models.

1c) In your final model in (b), which variables are useful in predicting
death due to heart failure? List the most important two variables

```{r}
summary(cvd_log_fit)
```

Age and ejection fraction appear to have the smallest p-values in the summary table, they appear to be the two most useful variables to predict death due to heart failure. However we cannot rely on this statistic alone, so we use variable important from the decision tree.

```{r}
dt_important = data.frame(importance = cvd_dt_fit$variable.importance)
ranking = dt_important %>% 
  tibble::rownames_to_column() %>% 
  dplyr::rename("variable" = rowname) %>% 
  dplyr::arrange(importance) %>%
  dplyr::mutate(variable = forcats::fct_inorder(variable))
ggplot2::ggplot(ranking) +
  geom_col(aes(x = variable, y = importance),
           col = "black", show.legend = F) +
  coord_flip() +
  scale_fill_grey() +
  theme_bw()
```

```{r}
imp = importance(cvd_rf_fit)
sort(imp[,1],decreasing=TRUE)
```

A decision tree works by selection the variable and decision boundary would would provide the greater information gain in separating classes death (1) and not death (0). Variables of higher importance are use for splits higher up on the decision tree. Looking at the variable importance graph, we can confirm that ejection fraction and age are the two most important variables to predict death caused by cvd. 

This is the same with random forests, which randomly excludes variables for each random tree. Despite this, ejection fraction and age still appear to be the two most important covariates.

1d) We have learnt various classification methods and, some classifiers are
only applicable to numeric explanatory variables. If we consider only
numeric explanatory variables, do we get a better classifer? Compare
classifiers using the same validation method in (b) and answer the
question

Use now only age, creatinine_phosphokinase, ejection_fraction, platelets, serum_sodium in our model. We can now use LDA, QDA and SVM and compare performance to logistic regression, decision tree and random forests. Let's compare their relative performances using the aforementioned evaluation metrics. Let's do an EDA first...

#### Exploratory Data Analysis

```{r}
exclude_cols <- c("death", "anaemia", "diabetes", "high_blood_pressure", "sex", "smoking")

legend_colors = c("red", "blue")
{
pairs(cvd_df[c("age", "creatinine_phosphokinase", "ejection_fraction", "platelets", "serum_sodium")],
      col = ifelse(cvd_df$death == 1, "red", "blue"),
      main = "Pairs Plot of Explanatory Variables and Death")

legend("topright", legend = c("Death", "Survived"), col = legend_colors, pch = 1)
}
```

```{r}
class_freq = table(cvd_df$death)

class_labels = recode(names(class_freq), "1" = "death", "0" = "survived")

ggplot(data.frame(Class = class_labels, Frequency = as.vector(class_freq)), aes(x = Class, y = Frequency, fill = Class)) +
  geom_bar(stat = "identity") +
  scale_fill_manual(values = c("red", "blue")) +
  xlab("Class") +
  ylab("Frequency") +
  ggtitle("Class Imbalance in the Dataset")
```


- There appears to be a somewhat clear separation vertical linear boundary between death and survival at the lower end of ejection fraction compared to all other explanatory variables. 

- Observations with a greater age appear to have a higher chance of dying due to CVD.

- Platelets and serum sodium vs other explanatory variables appear to have a appear to have slight quadratic decision boundaries between death and survival.

- No notable blobs in the pair plot.

- There appears to be a class imbalance between survived and dead observations, with the survived class having twice more observations.

#### LDA

```{r}
cvd_lda_fit = lda(death ~ age + creatinine_phosphokinase + ejection_fraction + platelets + serum_sodium, cvd_train_df)
cvd_lda_fit$means
```

```{r}
# plot boundaries
partimat(as.factor(death) ~ age + creatinine_phosphokinase + ejection_fraction + platelets + serum_sodium, data = cvd_train_df, method = "lda")
```

```{r}
#prediction 
death_lda_pred = predict(cvd_lda_fit, cvd_test_df)
death_lda_pred = as.data.frame(death_lda_pred)

#confusion matrix
lda_mx <- table(death_lda_pred$class, cvd_test_df$death)
lda_mx
```

```{r}
metrics_lda = measure.classification(death_lda_pred$class, cvd_test_df$death, lda_mx)
rownames(metrics_lda) = "LDA"

metric_comparison = rbind(metric_comparison, metrics_lda)
```

#### QDA

```{r}
cvd_qda_fit = qda(death ~ age + creatinine_phosphokinase + ejection_fraction + platelets + serum_sodium, cvd_train_df)
cvd_qda_fit$means
```

```{r}
# plot boundaries
partimat(as.factor(death) ~ age + creatinine_phosphokinase + ejection_fraction + platelets + serum_sodium, data = cvd_train_df, method = "qda")
```

```{r}
#prediction 
death_qda_pred = predict(cvd_qda_fit, cvd_test_df)
death_qda_pred = as.data.frame(death_qda_pred)

#confusion matrix
qda_mx = table(death_qda_pred$class, cvd_test_df$death)
qda_mx
```

```{r}
metrics_qda = measure.classification(death_qda_pred$class, cvd_test_df$death, qda_mx)
rownames(metrics_qda) = "QDA"

metric_comparison = rbind(metric_comparison, metrics_qda)
```

#### SVM (Linear Kernel)
```{r}
cvd_svm_linear_fit <- svm(as.factor(death) ~ age + creatinine_phosphokinase + ejection_fraction + platelets + serum_sodium, data = cvd_train_df, kernel = 'linear', probability = TRUE)

# most interesting plots
{
plot(cvd_svm_linear_fit, cvd_train_df, age ~ ejection_fraction)
plot(cvd_svm_linear_fit, cvd_train_df, age ~ creatinine_phosphokinase)
plot(cvd_svm_linear_fit, cvd_train_df, age ~ platelets)
plot(cvd_svm_linear_fit, cvd_train_df, age ~ serum_sodium)
plot(cvd_svm_linear_fit, cvd_train_df, creatinine_phosphokinase ~ ejection_fraction)
plot(cvd_svm_linear_fit, cvd_train_df, ejection_fraction ~ serum_sodium)
}
```

Age vs Ejection Fraction shows the strongest decision boundary to distinguish the positive and negative classes in absence of a distribution.

```{r}
death_svm_linear_pred = predict(cvd_svm_linear_fit, cvd_test_df)

svm_linear_mx = table(death_svm_linear_pred, cvd_test_df$death)
svm_linear_mx
```

```{r}
metrics_svm_linear = measure.classification(death_svm_linear_pred, cvd_test_df$death, svm_linear_mx)
rownames(metrics_svm_linear) = "SVM (Linear Kernel)"

metric_comparison = rbind(metric_comparison, metrics_svm_linear)
```

#### SVM (Polynomial Kernel)

```{r}
cvd_svm_poly_fit = svm(as.factor(death) ~ age + creatinine_phosphokinase + ejection_fraction + platelets + serum_sodium, data = cvd_train_df, kernel = 'polynomial', probability = TRUE)

{
plot(cvd_svm_poly_fit, cvd_train_df, age ~ ejection_fraction)
plot(cvd_svm_poly_fit, cvd_train_df, age ~ creatinine_phosphokinase)
plot(cvd_svm_poly_fit, cvd_train_df, age ~ platelets)
plot(cvd_svm_poly_fit, cvd_train_df, age ~ serum_sodium)
plot(cvd_svm_poly_fit, cvd_train_df, creatinine_phosphokinase ~ ejection_fraction)
plot(cvd_svm_poly_fit, cvd_train_df, ejection_fraction ~ serum_sodium)
}
```

```{r}
death_svm_poly_pred = predict(cvd_svm_poly_fit, cvd_test_df)

svm_poly_mx = table(death_svm_poly_pred, cvd_test_df$death)
svm_poly_mx
```

```{r}
metrics_svm_poly = measure.classification(death_svm_poly_pred, cvd_test_df$death, svm_poly_mx)
rownames(metrics_svm_poly) = "SVM (Polynomial Kernel)"

metric_comparison = rbind(metric_comparison, metrics_svm_poly)
```

#### SVM (Radial Kernel)

```{r}
cvd_svm_rad_fit = svm(as.factor(death) ~ age + creatinine_phosphokinase + ejection_fraction + platelets + serum_sodium, data = cvd_train_df, kernel = 'radial', probability = TRUE)

{
plot(cvd_svm_rad_fit, cvd_train_df, age ~ ejection_fraction)
plot(cvd_svm_rad_fit, cvd_train_df, age ~ creatinine_phosphokinase)
plot(cvd_svm_rad_fit, cvd_train_df, age ~ platelets)
plot(cvd_svm_rad_fit, cvd_train_df, age ~ serum_sodium)
plot(cvd_svm_rad_fit, cvd_train_df, creatinine_phosphokinase ~ ejection_fraction)
plot(cvd_svm_rad_fit, cvd_train_df, ejection_fraction ~ serum_sodium)
}
```

```{r}
death_svm_rad_pred = predict(cvd_svm_rad_fit, cvd_test_df)

svm_rad_mx = table(death_svm_rad_pred, cvd_test_df$death)
svm_rad_mx
```

```{r}
metrics_svm_rad = measure.classification(death_svm_rad_pred, cvd_test_df$death, svm_rad_mx)
rownames(metrics_svm_rad) = "SVM (Radial Kernel)"

metric_comparison = rbind(metric_comparison, metrics_svm_rad)
```

#### Comparisons
```{r}
head(metric_comparison, 8)
```

LDA performs the best out of the models in d, could be due to a simple distribution between the death and survived class with ejection fraction and age as the axis. More complicated classification models such as QDA performs worse, even though the decision boundary is more flexible, it is more likely to overfit on the training data. This is the same with all types of SVM as they rely on a small amount of support vectors to create the 'best' decision boundary to separate classes in the training data. However there appears to be quite a bit of variation in the data which would explain the overfit of these models to the training data, resulting in poor test accuracy.

Overall LDA performs just as well as decisions trees. Both models are good choices for this problem, however it depends on the purpose of the investigation. Decision trees are non-parametric making them distribution free and more flexible. Whereas LDA is a parametic model which assumes the same covariance matrix between classes. Regardless of this, both models have a high degree of interpretability.

_________________________________________________________________________________________________________________________________________________________________________________________________

The spreadsheet AckTemp.csv contain daily maximum temperature in
Auckland from 1 Jan 2014 to 30 June 2020. The attributes follow;

##### Feature Description

year; 2014, ..., 2020
month; 1,2,...,12
day; 1,2,...
maxT; (Maximum daily temperature in Celsius) 3, 6.7, 4.8 ... 6.7

#### Key objects of daily maximum temperature series are estimated naively
and are presented graphically. We use this as prestudy to model the long
term behaviour of daily max temperature.

#### From the prestudy (above figure), both seasonality and non-linear trend
are observed. We will treat month as a factor variable and assume fixed
month effect.

#### year and day are not sufficient to parametrize the long-term trend and,
we will introduce the new variable y which it is a vector of cumulative
days from 1 Jan 2014. For example, y=1 on 1 Jan 2014, ..., y=31 on 31
Jan 2014, ..., y=2283 on 30 June 2020.

#### 2a) Create the new data in which there are the three variables, y, month
and maxT

```{r}
akl_weather_df = read_csv("data/AckTemp.csv")
glimpse(akl_weather_df)
```
```{r}
akl_weather_df$datetime <- ymd(paste(akl_weather_df$year, akl_weather_df$month, akl_weather_df$day, sep = "-"))
reference_datetime <- ymd("2014-01-01")

akl_weather_df = akl_weather_df %>%
  mutate(y = as.numeric(difftime(akl_weather_df$datetime, reference_datetime, units = "days")) + 1)

akl_weather_df = akl_weather_df[,c("y", "month", "maxT")]
akl_weather_df$month = as.factor(akl_weather_df$month)

glimpse(akl_weather_df)
```

#### 2b) Find an appropriate non-parametric model for daily maximum tem-
perature using the data in (a). Write reason(s) for your choice of
model and degrees of freedom.

```{r}
plot(maxT ~ y, data = akl_weather_df, xlab = "Days Since 1-1-2014 (Days)", ylab = "Maximum Temperature (C)", main = "Maximum Temperatures in Auckland (1-1-2014 - 30-6-2020)")
```

There is seasonality and trend on inspection. The seasonality is based on seasons in New Zealand, so we would expect a cycle period of 365 days (1 year).

NOTE: Use CV
NOTE: create MSE plot
Maybe choose parsim model?

```{r}
```



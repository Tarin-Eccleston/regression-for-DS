catheter.df <- read.table("catheter.data", header = TRUE)
catheter.fit <- lm(ca ~ ht, data = catheter.df)
summary(catheter.fit)
# MUST Label Axis
plot(ca ~ ht, data = catheter.df, xlab = "Height (inches)", ylab = "Catheter Lenght (inches)")
abline(coef(catheter.fit)[1], coef(catheter.fit)[2])
source("~/Documents/Masters/STATS 762/Exercises/Catheter Analysis/catheter_analysis.R")
setwd("/Users/tarineccleston/Documents/Masters/STATS 762/Exercises/Catheter Analysis")
catheter.df <- read.table("catheter.data", header = TRUE)
catheter.fit <- lm(ca ~ ht, data = catheter.df)
summary(catheter.fit)
# MUST Label Axis
plot(ca ~ ht, data = catheter.df, xlab = "Height (inches)", ylab = "Catheter Lenght (inches)")
abline(coef(catheter.fit)[1], coef(catheter.fit)[2])
crabs.df <- read.table("crab.data", header = TRUE)
plot(sats ~ weight, data = crabs.df)
crabs.fit <- glm(sats ~ weight, data = crabs.df, family = "poisson")
summary(crabs.df)
xx <- seq(0, 6, length.out = 100)
yy <- predict(crabs.df, newdata = data.frame(weight = xx), type = "response")
lines(xx, yy)
chd.df <- read.table("chd.data", header = TRUE)
chd.fit <- glm(chd ~ age, family = "binomial", data = chd.df)
summary(chd.fit)
plot(chd ~ age, data = chd.df)
xx <- seq(10, 80, length.out = 1000)
yy <- predict(chd.fit, newdata = data.frame(age = xx), type = "response")
lines(xx, yy)
chd.df <- read.table("chd.data", header = TRUE)
chd.fit <- glm(chd ~ age, family = "binomial", data = chd.df)
summary(chd.fit)
plot(chd ~ age, data = chd.df)
xx <- seq(min(chd.df$age), max(chd.df$age), length.out = 1000)
yy <- predict(chd.fit, newdata = data.frame(age = xx), type = "response")
lines(xx, yy)
crabs.df <- read.table("crab.data", header = TRUE)
plot(sats ~ weight, data = crabs.df)
# Use linear regression. Poisson regression is most suitable in this case.
# The expected value must be above 0, so linear regression doesn't give meaningful prediction sometimes.
crabs.fit <- lm(sats ~ weight, data = crabs.df)
plot(sats ~ weight, data = crabs.df)
abline(crabs.fit$coefficients[1], crabs.fit$coefficients[2])
# Poisson regression
crabs.logfit <- glm(sats ~ weight, data = crabs.df, family = "poisson")
# We cannot plot curve in R, so use sequence.
xx <- seq(min(crabs.df$weight), max(crabs.df$weight), length.out = 101)
# The linear predictor gives log of mew. Log being the link function. To get mew, don't need to exponentiate, specify type as response.
yy <- predict(crabs.logfit, newdata = data.frame(weight = xx), type = 'response')
plot(sats ~ weight, data = crabs.df)
lines(yy ~ xx)
chd.fit <- glm(chd ~ age, family = "binomial", data = chd.df)
summary(chd.fit)
plot(chd ~ age, data = chd.df)
xx <- seq(10, 80, length.out = 1000)
yy <- predict(chd.fit, newdata = data.frame(age = xx), type = "response")
lines(xx, yy)
# Create new matrix, age from sample, and predicted likelihood of chd to the response
cbind(chd.df$age, fitted(chd.fit))
# flipping "coin" for each person from age 20 -> 63
n.people <- nrow(chd.df)
new.chd <- rbinom(n.people, 1, predict(chd.fit, type = "response"))
par(mfrow = c(1, 2))
plot(chd ~ age, data = chd.df, main = "Original")
yy <- predict(chd.fit, newdata = data.frame(age = xx), type = "response")
lines(xx, yy)
plot(new.chd ~ age, data = chd.df, main = "Simulated")
new.fit <- glm(new.chd ~ age, family = binomial("probit"), data = chd.df)
yy <- predict(new.fit, newdata = data.frame(age = xx), type = "response")
lines(xx, yy)
git
git init
n.boots <- 1000
coef.boot <- matrix(0, nrow = n.boots, ncol = 2)
agep80.boot <- numeric(n.boots)
for (i in 1:n.boots){
chd.boot <- rbinom(n.people, 1, predict(chd.fit, type = "response"))
fit.boot <- glm(chd.boot ~ age, type = "binomial", data = chd.df)
coef.boot[i, ] <- coef(fit.boot)
agep80.boot[i] <- (0.84162 - coef(fit.boot)[1])/coef(fit.boot)[2]
}
n.boots <- 1000
coef.boot <- matrix(0, nrow = n.boots, ncol = 2)
agep80.boot <- numeric(n.boots)
for (i in 1:n.boots){
chd.boot <- rbinom(n.people, 1, predict(chd.fit, type = "response"))
fit.boot <- glm(chd.boot ~ age, family = "binomial", data = chd.df)
coef.boot[i, ] <- coef(fit.boot)
agep80.boot[i] <- (0.84162 - coef(fit.boot)[1])/coef(fit.boot)[2]
}
summary(fit.boot)
apply(coef.boot, 2, sd)
git help
setwd("/Users/tarineccleston/Documents/Masters/STATS 765/statistical-learning-for-DS/var-selection-cross-valid")
library(tidyverse)
sgemm_df = read.csv("data/sgemm_product.csv")
```{r}
set.seed(991920861)
sgemm_sample_df = sgemm_df[sample(nrow(sgemm_df), 500), ]
View(sgemm_sample_df)
sgemm_subset_df = sgemm_df[sample(nrow(sgemm_df), 500), ]
View(sgemm_subset_df)
library(leaps)
```{r}
set.seed(991920861)
sgemm_subset_indices_df = sample(1:nrow(sgemm_df), 500)
```{r}
sgemm_df = sgemm_df %>%
mutate(Log_Run1..ms. = log(Run1..ms.))
View(sgemm_df)
```{r}
sgemm_run1_df = sgemm_df %>%
mutate(Log_Run1..ms. = log(Run1..ms.)) %>%
select(-Run1..ms., -Run2..ms., -Run3..ms., -Run4..ms.)
View(sgemm_run1_df)
X<-as.matrix(sgemm_run1_df[,-1])
View(X)
X<-as.matrix(sgemm_run1_df%>%select(-Log_Run1..ms.))
View(X)
y<-sgemm_run1_df$Log_Run1..ms.
fsearch <- regsubsets(X, y, nvmax=20, method="back")
View(fsearch)
?regsubsets
fsearch <- regsubsets(Log_Run1..ms. ~ .^2, data = sgemm_run1_df, nvmax=20, method="back")
View(fsearch)
plot(model$rss, xlab = "Number of Predictors", ylab = "Residual Sum of Squares")
model = regsubsets(Log_Run1..ms. ~ .^2, data = sgemm_run1_df, nvmax=20, method="back")
summary(model)
model = regsubsets(Log_Run1..ms. ~ .^2, data = sgemm_run1_df, nvmax=20, method="backward")
summary(model)
plot(model$rss, xlab = "Number of Predictors", ylab = "Residual Sum of Squares")
my_plot <- ggplot(data = model, aes(x = model, y = model$rss)) +
geom_point() + # add the points to the plot
geom_line()    # add the line to the plot
rss
my_plot <- ggplot(data = model$rss, aes(x = model, y = model$rss)) +
geom_point() + # add the points to the plot
geom_line()    # add the line to the plot
plot(model$rss, xlab = "Number of Predictors", ylab = "Residual Sum of Squares")
labs("Number of Predictors vs Residual Sum of Squares")
title("Number of Predictors vs Residual Sum of Squares")
plot(model$rss, xlab = "Number of Predictors", ylab = "Residual Sum of Squares")
title("Number of Predictors vs Residual Sum of Squares")
{plot(model$rss, xlab = "Number of Predictors", ylab = "Residual Sum of Squares")
title("Number of Predictors vs Residual Sum of Squares")}
{plot(model$rss, xlab = "Number of Predictors", ylab = "Residual Sum of Squares")
title("Number of Predictors vs Residual Sum of Squares")
for (i in 1:(length(x)-1)) {
lines(c(x[i], x[i+1]), c(y[i], y[i+1]), col = "red")
}}
{plot(model$rss, xlab = "Number of Predictors", ylab = "Residual Sum of Squares")
title("Number of Predictors vs Residual Sum of Squares")}
folds=sample(rep(1:10,length.out=n))
lambdas<-c(1,2,4,6,8,10,12)# candidate sets for \lambda fitted1<-matrix(NA,nrow=n,ncol=length(lambdas))
folds=sample(rep(1:10,length.out=n))
folds=sample(rep(1:10,length.out=n))
lambdas<-c(1,2,4,6,8,10,12)
fitted1<-matrix(NA,nrow=n,ncol=length(lambdas))
?rep
fitted1<-matrix(nrow=10,ncol=length(lambdas))
lambdas = (2,4,6,8,10,12)
lambdas = c(2,4,6,8,10,12)
fitted = matrix(nrow=10,ncol=length(lambdas))
folds=sample(rep(1:10,length.out=n))
lambdas = c(2,4,6,8,10,12)
fitted = matrix(nrow=10,ncol=length(lambdas))
for(k in 1:10){
train = (1:n)[folds!=k]
test = (1:n)[folds==k]
fitted[test,]<-allyhat(xtrain=X[train,],ytrain=y[train],xtest=X[test,],lambdas)
}
View(fitted)
folds=sample(rep(1:10,10))
lambdas = c(2,4,6,8,10,12)
fitted = matrix(nrow=10,ncol=length(lambdas))
for(k in 1:10){
train = (1:n)[folds!=k]
test = (1:n)[folds==k]
fitted[test,]<-allyhat(xtrain=X[train,],ytrain=y[train],xtest=X[test,],lambdas)
}
allyhat<-function(xtrain, ytrain, xtest, lambdas, nvmax=50){
n<-nrow(xtrain)
yhat<-matrix(nrow=nrow(xtest),ncol=length(lambdas))
search<-regsubsets(xtrain,ytrain, nvmax=nvmax, method="back")
summ<-summary(search)
for(i in 1:length(lambdas)){
penMSE<- n*log(summ$rss)+lambdas[i]*(1:nvmax)
best<-which.min(penMSE)  #lowest AIC
betahat<-coef(search, best) #coefficients
xinmodel<-cbind(1,xtest)[,summ$which[best,]] #predictors in that model
yhat[,i]<-xinmodel%*%betahat
}
yhat
}
n = length(sgemm_run1_df)
folds=sample(rep(1:10,length.out=n)
n = length(sgemm_run1_df)
folds=sample(rep(1:10,length.out=n)
folds=sample(rep(1:10,length.out=n) )
n = length(sgemm_run1_df)
folds=sample(rep(1:10,length.out=n))
lambdas = c(2,4,6,8,10,12)
fitted = matrix(nrow=10,ncol=length(lambdas))
lambdas = c(2,4,6,8,10,12)
fitted = matrix(nrow=n,ncol=length(lambdas))
n = nrow(sgemm_run1_df)
folds=sample(rep(1:10,length.out=n))
lambdas = c(2,4,6,8,10,12)
fitted = matrix(nrow=n,ncol=length(lambdas))
folds=sample(rep(1:10,10))
lambdas = c(2,4,6,8,10,12)
fitted = matrix(nrow=10,ncol=length(lambdas))
View(fitted)
for(k in 1:10){
train = (1:n)[folds!=k]
test = (1:n)[folds==k]
fitted[test,]<-allyhat(xtrain=X[train,],ytrain=y[train],xtest=X[test,],lambdas)
}
for(k in 1:10){
train = (1:n)[folds!=k]
test = (1:n)[folds==k]
fitted[test,]<-allyhat(xtrain=X[train,],ytrain=y[train],xtest=X[test,],lambdas)
}
folds
glimpse(sgemm_run1_df)
# get sample
sgemm_run1_df = sgemm_run1_df[sgemm_subset_indices_df,]
glimpse(sgemm_run1_df)
X<-model.matrix(Log_Run1..ms.~.^2, sgemm_run1_df)
X<-model.matrix(Log_Run1..ms.~.^2, sgemm_run1_df)[,-1]
y<-sgemm_run1_df$Log_Run1..ms.
model = regsubsets(X, y, nvmax=20, method="backward")
X<-model.matrix(Log_Run1..ms.~.^2, sgemm_run1_df)[,-1]
y<-sgemm_run1_df$Log_Run1..ms.
model = regsubsets(X, y, nvmax=20, method="backward")
summary(model)
X
View(X)
model = regsubsets(X, y, nvmax=20, method="backward")
View(model)
model = regsubsets(X, y, nvmax=20, method="backward")
subset = summary(model)
apparentErrors = subset$rss / (nrow(sgemm_run1_df) - 1:20)
qplot(x = 1:20, y = apparentErrors)
ggplot(x = 1:20, y = apparentErrors)
qplot(x = 1:20, y = apparentErrors)
qplot(x = Folds, y = apparentErrors)
qplot(x = 1:20, y = apparentErrors, xlab = Folds)
qplot(x = 1:20, y = apparentErrors, xlab = "Folds")
qplot(x = 1:20, y = apparentErrors, xlab = "Number of Variables", ylab = "Apparent Error", title("Number of Variuables vs Apparent Errors"))
qplot(x = 1:20, y = apparentErrors, xlab = "Number of Variables", ylab = "Apparent Error", title = "Number of Variuables vs Apparent Errors")
qplot(x = 1:20, y = apparentErrors, xlab = "Number of Variables", ylab = "Apparent Error")
y = sgemm_run1_df$Log_Run1..ms.
n = nrow(X)
folds=sample(rep(1:10,length.out=n))
lambdas = c(2,4,6,8,10,12)
fitted = matrix(nrow=n,ncol=length(lambdas))
fitted = matrix(nrow=n,ncol=length(lambdas))
for(k in 1:10){
train = (1:n)[folds!=k]
test = (1:n)[folds==k]
fitted[test,]<-allyhat(xtrain=X[train,],ytrain=y[train],xtest=X[test,],lambdas)
}
a=colMeans((y-fitted)^2)
a
lambdas[which.min(a)]
```{r}
sgemm_run2_df = sgemm_df %>%
mutate(Log_Run2..ms. = log(Run2..ms.)) %>%
select(-Run1..ms., -Run2..ms., -Run3..ms., -Run4..ms.)
View(sgemm_run2_df)
sgemm_df = read.csv("data/sgemm_product.csv")
```{r}
sgemm_run2_df = sgemm_df %>%
mutate(Log_Run2..ms. = log(Run2..ms.)) %>%
select(-Run1..ms., -Run2..ms., -Run3..ms., -Run4..ms.)
sgemm_run2_df = sgemm_run2_df[sgemm_subset_indices_df,]
sgemm_run2_df = sgemm_run2_df[sgemm_subset_indices_df,]
best_lambda = 8
search_run2 = regsubsets(X, y, nvmax = 20, method = "backwards")
summ = summary(search_run2)
X<-model.matrix(Log_Run1..ms.~.^2, sgemm_run1_df)[,-1]
y<-sgemm_run1_df$Log_Run1..ms.
search_run2 = regsubsets(X, y, nvmax = 20, method = "backwards")
search_run2 = regsubsets(X, y, nvmax = 20, method = "backward")
summ = summary(search_run2)
aic = length(sgemm_subset_indices_df) * log(summ$rss) + best_lambda*(1:20)
best = which.min(aic)
betahat = coef(search, best)
betahat = coef(search_run2, best)
betahat
```{r}
Xpred = cbind(1, X)[,summ$which[best,]]
fitted = Xpred%*%betahat
MSPEExample = sum((sgemm_run2_df - fitted)^2) / length(fitted)
MSPEExample
setwd("/Users/tarineccleston/Documents/Software:DS/predict-flight-delays")
library(tidyverse)
library(skimr)
load("../output/flights.RData")
install.packages(cowplot)
install.packages("cowplot")
setwd("/Users/tarineccleston/Documents/Masters/STATS 762/regression-for-DS/bootstrapping")
# a bit of a throwback
chd_df = read.table("data/chd.data", header = TRUE)
chd_fit = glm(chd ~ age, family = "binomial", data = chd_df)
summary(chd_fit)
plot(chd ~ age, data = chd_df)
xx = seq(10, 80, length.out = 1000)
yy = predict(chd_fit, newdata = data.frame(age = xx), type = "response")
lines(xx, yy)
# Create new matrix, age from sample, and predicted likelihood of chd to the response
cbind(chd_df$age, fitted(chd_fit))
# flipping "coin" for each person from age 20 -> 63
n_people = nrow(chd_df)
new_chd = rbinom(n_people, 1, predict(chd_fit, type = "response"))
# comparison between real and simulated responce
par(mfrow = c(1, 2))
plot(chd ~ age, data = chd_df, main = "Original")
yy = predict(chd_fit, newdata = data.frame(age = xx), type = "response")
lines(xx, yy)
plot(new_chd ~ age, data = chd_df, main = "Simulated")
# fit binomial model
new_fit = glm(new_chd ~ age, family = binomial("probit"), data = chd_df)
yy = predict(new_fit, newdata = data.frame(age = xx), type = "response")
lines(xx, yy)
# create multiple simulations to calculate estimates for int and co-eff, and then calculate SE, SD
# can run simulations instead of calculating values using maths/theory
n_boots = 10000
coef_boot = matrix(0, nrow = n_boots, ncol = 2)
# vector to store estimates of the age at which the chance of having CHD is 80%
agep80_boot = numeric(n_boots)
for (i in 1:n_boots){
chd_boot = rbinom(n_people, 1, predict(chd_fit, type = "response"))
fit_boot = glm(chd_boot ~ age, family = binomial(link = "probit"), data = chd_df)
coef_boot[i, ] = coef(fit_boot)
agep80_boot[i] = (0.84162 - coef(fit_boot)[1])/coef(fit_boot)[2]
}
# SE from summary()
summary(chd_fit)$coef[,2]
# SE from the boostrap
apply(coef_boot, 2, sd)
# create multiple simulations to calculate estimates for int and co-eff, and then calculate SE, SD
# can run simulations instead of calculating values using maths/theory
n_boots = 10000
coef_boot = matrix(0, nrow = n_boots, ncol = 2)
# vector to store estimates of the age at which the chance of having CHD is 80%
agep80_boot = numeric(n_boots)
for (i in 1:n_boots){
chd_boot = rbinom(n_people, 1, predict(chd_fit, type = "response"))
fit_boot = glm(chd_boot ~ chd_df$age, family = binomial(link = "probit"))
coef_boot[i, ] = coef(fit_boot)
agep80_boot[i] = (0.84162 - coef(fit_boot)[1])/coef(fit_boot)[2]
}
# SE from summary()
summary(chd_fit)$coef[,2]
# SE from the boostrap
apply(coef_boot, 2, sd)
# estimate from the real data
agep80_est = (0.84162 - coef(chd_fit)[1]/coef(chd_fit)[2])
agep80_est
# the standard error calculated by bootstrap
agep80_se = sd(agep80_boot)
summary(chd_fit)
